{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-20T21:59:50.454783Z",
     "start_time": "2025-03-20T21:59:50.452590Z"
    }
   },
   "source": [
    "from TTS.api import TTS\n",
    "import torch\n",
    "\n",
    "\n",
    "import simpleaudio as sa"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T21:59:37.417114Z",
     "start_time": "2025-03-20T21:56:47.836366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(TTS().list_models())\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)"
   ],
   "id": "699a0de98ed28c60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TTS.utils.manage.ModelManager object at 0x321f346d0>\n",
      " > You must confirm the following:\n",
      " | > \"I have purchased a commercial license from Coqui: licensing@coqui.ai\"\n",
      " | > \"Otherwise, I agree to the terms of the non-commercial CPML: https://coqui.ai/cpml\" - [y/n]\n",
      " > Downloading model to /Users/dogukangundogan/Library/Application Support/tts/tts_models--multilingual--multi-dataset--xtts_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1.87G/1.87G [02:19<00:00, 13.6MiB/s]\n",
      "100%|██████████| 1.87G/1.87G [02:19<00:00, 13.3MiB/s]\n",
      "100%|██████████| 4.37k/4.37k [00:00<00:00, 9.80kiB/s]\n",
      " 55%|█████▍    | 198k/361k [00:00<00:00, 1.41MiB/s]\n",
      "100%|██████████| 361k/361k [00:00<00:00, 1.05MiB/s]\n",
      "100%|██████████| 32.0/32.0 [00:00<00:00, 135iB/s]\n",
      " 90%|████████▉ | 6.94M/7.75M [00:00<00:00, 13.7MiB/s]/Users/dogukangundogan/Library/Caches/pypoetry/virtualenvs/nexai-backend-rybVfxlQ-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Model's license - CPML\n",
      " > Check https://coqui.ai/cpml.txt for more info.\n",
      " > Using model: xtts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7.75M/7.75M [00:19<00:00, 13.7MiB/s]"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001B[1mdo those steps only if you trust the source of the checkpoint\u001B[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL TTS.tts.configs.xtts_config.XttsConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([XttsConfig])` or the `torch.serialization.safe_globals([XttsConfig])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mUnpicklingError\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m device = \u001B[33m\"\u001B[39m\u001B[33mcuda\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch.cuda.is_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28mprint\u001B[39m(TTS().list_models())\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m tts = \u001B[43mTTS\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtts_models/multilingual/multi-dataset/xtts_v2\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m.to(device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/nexai-backend-rybVfxlQ-py3.11/lib/python3.11/site-packages/TTS/api.py:74\u001B[39m, in \u001B[36mTTS.__init__\u001B[39m\u001B[34m(self, model_name, model_path, config_path, vocoder_path, vocoder_config_path, progress_bar, gpu)\u001B[39m\n\u001B[32m     72\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m model_name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(model_name) > \u001B[32m0\u001B[39m:\n\u001B[32m     73\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mtts_models\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m model_name:\n\u001B[32m---> \u001B[39m\u001B[32m74\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mload_tts_model_by_name\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgpu\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     75\u001B[39m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mvoice_conversion_models\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m model_name:\n\u001B[32m     76\u001B[39m         \u001B[38;5;28mself\u001B[39m.load_vc_model_by_name(model_name, gpu)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/nexai-backend-rybVfxlQ-py3.11/lib/python3.11/site-packages/TTS/api.py:177\u001B[39m, in \u001B[36mTTS.load_tts_model_by_name\u001B[39m\u001B[34m(self, model_name, gpu)\u001B[39m\n\u001B[32m    171\u001B[39m model_path, config_path, vocoder_path, vocoder_config_path, model_dir = \u001B[38;5;28mself\u001B[39m.download_model_by_name(\n\u001B[32m    172\u001B[39m     model_name\n\u001B[32m    173\u001B[39m )\n\u001B[32m    175\u001B[39m \u001B[38;5;66;03m# init synthesizer\u001B[39;00m\n\u001B[32m    176\u001B[39m \u001B[38;5;66;03m# None values are fetch from the model\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m177\u001B[39m \u001B[38;5;28mself\u001B[39m.synthesizer = \u001B[43mSynthesizer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    178\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtts_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    179\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtts_config_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    180\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtts_speakers_file\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    181\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtts_languages_file\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    182\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvocoder_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvocoder_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    183\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvocoder_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvocoder_config_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    184\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    185\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_config\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    186\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    187\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cuda\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgpu\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    188\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/nexai-backend-rybVfxlQ-py3.11/lib/python3.11/site-packages/TTS/utils/synthesizer.py:109\u001B[39m, in \u001B[36mSynthesizer.__init__\u001B[39m\u001B[34m(self, tts_checkpoint, tts_config_path, tts_speakers_file, tts_languages_file, vocoder_checkpoint, vocoder_config, encoder_checkpoint, encoder_config, vc_checkpoint, vc_config, model_dir, voice_dir, use_cuda)\u001B[39m\n\u001B[32m    107\u001B[39m     \u001B[38;5;28mself\u001B[39m.output_sample_rate = \u001B[38;5;28mself\u001B[39m.tts_config.audio[\u001B[33m\"\u001B[39m\u001B[33msample_rate\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    108\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m109\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_load_tts_from_dir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_cuda\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    110\u001B[39m     \u001B[38;5;28mself\u001B[39m.output_sample_rate = \u001B[38;5;28mself\u001B[39m.tts_config.audio[\u001B[33m\"\u001B[39m\u001B[33moutput_sample_rate\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/nexai-backend-rybVfxlQ-py3.11/lib/python3.11/site-packages/TTS/utils/synthesizer.py:164\u001B[39m, in \u001B[36mSynthesizer._load_tts_from_dir\u001B[39m\u001B[34m(self, model_dir, use_cuda)\u001B[39m\n\u001B[32m    162\u001B[39m \u001B[38;5;28mself\u001B[39m.tts_config = config\n\u001B[32m    163\u001B[39m \u001B[38;5;28mself\u001B[39m.tts_model = setup_tts_model(config)\n\u001B[32m--> \u001B[39m\u001B[32m164\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtts_model\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheckpoint_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43meval\u001B[39;49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    165\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m use_cuda:\n\u001B[32m    166\u001B[39m     \u001B[38;5;28mself\u001B[39m.tts_model.cuda()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/nexai-backend-rybVfxlQ-py3.11/lib/python3.11/site-packages/TTS/tts/models/xtts.py:771\u001B[39m, in \u001B[36mXtts.load_checkpoint\u001B[39m\u001B[34m(self, config, checkpoint_dir, checkpoint_path, vocab_path, eval, strict, use_deepspeed, speaker_file_path)\u001B[39m\n\u001B[32m    767\u001B[39m     \u001B[38;5;28mself\u001B[39m.tokenizer = VoiceBpeTokenizer(vocab_file=vocab_path)\n\u001B[32m    769\u001B[39m \u001B[38;5;28mself\u001B[39m.init_models()\n\u001B[32m--> \u001B[39m\u001B[32m771\u001B[39m checkpoint = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_compatible_checkpoint_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    773\u001B[39m \u001B[38;5;66;03m# deal with v1 and v1.1. V1 has the init_gpt_for_inference keys, v1.1 do not\u001B[39;00m\n\u001B[32m    774\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/nexai-backend-rybVfxlQ-py3.11/lib/python3.11/site-packages/TTS/tts/models/xtts.py:714\u001B[39m, in \u001B[36mXtts.get_compatible_checkpoint_state_dict\u001B[39m\u001B[34m(self, model_path)\u001B[39m\n\u001B[32m    713\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_compatible_checkpoint_state_dict\u001B[39m(\u001B[38;5;28mself\u001B[39m, model_path):\n\u001B[32m--> \u001B[39m\u001B[32m714\u001B[39m     checkpoint = \u001B[43mload_fsspec\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcpu\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m[\u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    715\u001B[39m     \u001B[38;5;66;03m# remove xtts gpt trainer extra keys\u001B[39;00m\n\u001B[32m    716\u001B[39m     ignore_keys = [\u001B[33m\"\u001B[39m\u001B[33mtorch_mel_spectrogram_style_encoder\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mtorch_mel_spectrogram_dvae\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mdvae\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/nexai-backend-rybVfxlQ-py3.11/lib/python3.11/site-packages/TTS/utils/io.py:54\u001B[39m, in \u001B[36mload_fsspec\u001B[39m\u001B[34m(path, map_location, cache, **kwargs)\u001B[39m\n\u001B[32m     52\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     53\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m fsspec.open(path, \u001B[33m\"\u001B[39m\u001B[33mrb\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[32m---> \u001B[39m\u001B[32m54\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/nexai-backend-rybVfxlQ-py3.11/lib/python3.11/site-packages/torch/serialization.py:1470\u001B[39m, in \u001B[36mload\u001B[39m\u001B[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[39m\n\u001B[32m   1462\u001B[39m                 \u001B[38;5;28;01mreturn\u001B[39;00m _load(\n\u001B[32m   1463\u001B[39m                     opened_zipfile,\n\u001B[32m   1464\u001B[39m                     map_location,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1467\u001B[39m                     **pickle_load_args,\n\u001B[32m   1468\u001B[39m                 )\n\u001B[32m   1469\u001B[39m             \u001B[38;5;28;01mexcept\u001B[39;00m pickle.UnpicklingError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m-> \u001B[39m\u001B[32m1470\u001B[39m                 \u001B[38;5;28;01mraise\u001B[39;00m pickle.UnpicklingError(_get_wo_message(\u001B[38;5;28mstr\u001B[39m(e))) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1471\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m _load(\n\u001B[32m   1472\u001B[39m             opened_zipfile,\n\u001B[32m   1473\u001B[39m             map_location,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1476\u001B[39m             **pickle_load_args,\n\u001B[32m   1477\u001B[39m         )\n\u001B[32m   1478\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m mmap:\n",
      "\u001B[31mUnpicklingError\u001B[39m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001B[1mdo those steps only if you trust the source of the checkpoint\u001B[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL TTS.tts.configs.xtts_config.XttsConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([XttsConfig])` or the `torch.serialization.safe_globals([XttsConfig])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "wav = tts.tts(text=\"Hello world!\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\")\n",
   "id": "a569ff55db1614ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7df316c9975e6354"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
